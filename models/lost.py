"""Adapted from https://arxiv.org/pdf/2109.14279.pdf (Section 3.2)"""

import os
import argparse
import torch
from torch import nn
from PIL import Image
import torchvision.transforms as T
import matplotlib.pyplot as plt
import cv2
import numpy as np
from tqdm import tqdm
from model import get_model, forward_dino_v1, show_attn, get_seed_from_attn

# don't change to utils, there is a utils.py elsewhere
from tools import unravel_index


class Lost(nn.Module):
    """Lost module"""
    def __init__(self, model, alpha, k=100):
        super().__init__()
        self.k = k  # indicates the cardinality of the seed expansion set
        self.model = model  # DINO model
        self.alpha = alpha  # alpha = 1 means that the seed is the pixel with the lowest degree
        # alpha = 0 means that the seed is the barycenter of the thresholded
        # (otsu) attention map
        assert 0 <= self.alpha <= 1, "alpha must be between 0 and 1"
        self.model.eval()

    def forward(self, img):
        """
        Args:
        img : input image (tensor) of shape (1,3,H,W)

        Output:
        mask : mask of shape (1,height_d,width_d) indicating the pixels
        that are part of the seed expansion set
        height_d and width_d are the dimensions of the patched image
        """
        height_d, width_d = img.shape[2] // 16, img.shape[3] // 16
        assert img.shape[2] % 16 == 0 and img.shape[3] % 16 == 0, "image dimensions must be divisible by 16"
        # run through the model
        with torch.inference_mode():
            out = forward_dino_v1(self.model, img).squeeze(0)
            # remove cls token
            out = out[1:]  # (height_d*width_d, D)
            attn = show_attn(self.model, img, is_v2=False)

        # get attention seed
        attn_seed = get_seed_from_attn(attn)  # (2,)

        # compute similarity matrix, degree matrix
        similarity_matrix = torch.matmul(
            out, out.T)              # (height_d*width_d, height_d*width_d)

        # (height_d*width_d, height_d*width_d)
        degree_matrix = similarity_matrix >= 0
        degree_matrix = degree_matrix.type(
            torch.int64)           # (height_d*width_d, height_d*width_d)
        # select seed with lowest degree
        seed_degree = torch.argmin(
            degree_matrix.sum(
                dim=0))  # or dim = 1, doesn't matter
        # (without loss of generality, we make a choice here)

        # the seed is a convex combination of the attention seed and the seed
        # with lowest degree

        # unravel coordinates
        seed_degree = unravel_index(seed_degree, (height_d, width_d))  # (2,)
        # attn seed is already in (y,x) format

        # compute seed
        seed_degree = seed_degree.to("cpu")
        attn_seed = attn_seed.to("cpu")

        seed = self.alpha * seed_degree + (1 - self.alpha) * attn_seed  # (2,)
        seed = torch.round(seed).type(torch.int64)  # (2,)

        # convert seed to index
        seed = seed[0] * width_d + seed[1]  # (1,)

        # expand seed set on similarity matrix
        degree_matrix[seed][seed] = 255
        set_seed = degree_matrix[seed]                            # (height_d*width_d,)

        # limit cardinality of seed expansion set to k
        ordered_set_seed = torch.argsort(
            similarity_matrix[seed],
            descending=True)  # returns indices
        # (height_d*width_d,)
        set_seed[ordered_set_seed[self.k:]] = 0

        # box extraction algorithm
        for i in range(len(set_seed)):
            if set_seed[i] == 0:
                continue
            if torch.sum(similarity_matrix[i][set_seed > 0]) > 0:
                # if the sum of the similarities between the current pixel
                # and the pixels in the seed expansion set is > 0
                # set the pixel to the sum of the similarities
                set_seed[i] = similarity_matrix[i][set_seed == 1].sum()
            else:
                set_seed[i] = 0

        # normalize
        set_seed = (set_seed - set_seed.min()) / \
            (set_seed.max() - set_seed.min())
        set_seed = set_seed.reshape(height_d, width_d).detach().cpu().numpy()
        set_seed = np.uint8(set_seed * 255)

        return set_seed


def main(n, is_grid, seed):
    """Main function"""

    if seed > 0:
        np.random.seed(seed)

    res = 224

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = get_model(size="s", use_v2=False)  # loads a DINOv1 model, size s
    model.to(device)

    k = 400 if is_grid else 100  # 4 images so 400 patches
    up = 2  # upscaling factor
    k = k * up**2  # logical to keep the same ratio patches/seed expansion set
    lost = Lost(model, alpha=0., k=k)

    for index in tqdm(range(n)):

        root = "/nasbrain/datasets/imagenet/images/val/"
        folder = np.random.choice(os.listdir(root))
        path = os.path.join(root, folder)

        # concatenate 4 images (grid)
        if is_grid:
            blank = Image.new("RGB", (res * 2, res * 2))

            for i, img_path in enumerate(os.listdir(path)[:4]):
                img = Image.open(os.path.join(path, img_path)).convert("RGB")
                #img = T.CenterCrop(res)(img)
                img = T.Resize((res, res), antialias=True)(img)
                blank.paste(img, (res * (i % 2), res * (i // 2)))

            img = blank
        else:
            name = np.random.choice(os.listdir(path))
            img = Image.open(os.path.join(path, name)).convert("RGB")
            #img = T.CenterCrop(res)(img)
            img = T.Resize((res, res), antialias=True)(img)

        # img.save(f"temp/img_{index}.png")
        plt.figure(figsize=(10, 5))

        width, height = img.size

        width, height = int(width * up), int(height * up)
        img = T.Resize((height // 16 * 16, width // 16 * 16), antialias=True)(img)
        img_t = T.ToTensor()(img).unsqueeze(0).to(device)

        out = lost(img_t)

        out = cv2.resize(out.astype(np.uint8), (width, height),
                         interpolation=cv2.INTER_NEAREST)

        # get the largest connected component amongst the non-zero pixels

        components = cv2.connectedComponentsWithStats(out, connectivity=4)

        _, labels, stats, _ = components

        # identify the background label
        background_indexes = np.where(out == 0)
        background_label = np.median(labels[background_indexes])

        # sort the labels by area
        sorted_labels = np.argsort(stats[:, cv2.CC_STAT_AREA])[::-1]
        # get the largest connected component
        largest_component_label = sorted_labels[0] if sorted_labels[0] != background_label else sorted_labels[1]

        # get the mask of the largest connected component
        mask = np.where(
            labels == largest_component_label,
            1,
            0).astype(
            np.uint8)

        plt.subplot(1, 3, 1)
        plt.imshow(img)
        plt.axis("off")
        plt.title("Image")

        plt.subplot(1, 3, 2)
        plt.imshow(out)
        plt.axis("off")
        plt.title("Lost output")

        plt.subplot(1, 3, 3)
        plt.imshow(mask)
        plt.axis("off")
        plt.title("Mask : largest connected component")

        plt.savefig(f"temp/lost_{index}.png")

        plt.close()

    print("Done")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--n", "-n", type=int, default=5,
                        help="Number of images to process")
    parser.add_argument(
        "--grid",
        action=argparse.BooleanOptionalAction,
        default=False,
        help="Whether to process a grid of images")
    parser.add_argument(
        "--seed",
        "-s",
        type=int,
        default=-1,
        help="Random seed, set to -1 for no seed")
    n = parser.parse_args().n
    is_grid = parser.parse_args().grid
    seed = parser.parse_args().seed
    main(n, is_grid, seed)
